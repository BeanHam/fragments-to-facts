{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9c7ee2ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/binhan/anaconda3/lib/python3.11/site-packages/bitsandbytes/cextension.py:34: UserWarning: The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers, 8-bit multiplication, and GPU quantization are unavailable.\n",
      "  warn(\"The installed version of bitsandbytes was compiled without GPU support. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'NoneType' object has no attribute 'cadam32bit_grad_fp32'\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from together import Together\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "from utils import *\n",
    "from huggingface_hub import login as hf_login\n",
    "from peft import prepare_model_for_kbit_training\n",
    "from datasets import concatenate_datasets, DatasetDict, load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, AutoModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "09a052b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROMPT_TO_USE = 0\n",
    "def add_world_model_probs(value, y_star, y_non_star, prompt, client, world_model_endpoints, max_tokens):\n",
    "    if 'world_models' not in value['y_stars'][y_star]:\n",
    "        value['y_stars'][y_star]['world_models'] = []\n",
    "    if 'world_models' not in value['y_NON_stars'][y_non_star]:\n",
    "        value['y_NON_stars'][y_non_star]['world_models'] = []\n",
    "        \n",
    "    for wm_endpoint in tqdm(world_model_endpoints, desc=\"world model endpoints\", leave=False):\n",
    "        wm_prob_gen = GenerateNextTokenProbAPI(client, wm_endpoint)\n",
    "        wm_prob_star = compute_token_probs_api(y_star, prompt, wm_prob_gen, max_tokens)\n",
    "        wm_prob_non_star = compute_token_probs_api(y_non_star, prompt, wm_prob_gen, max_tokens)\n",
    "        value['y_stars'][y_star]['world_models'].append(float(wm_prob_star))\n",
    "        value['y_NON_stars'][y_non_star]['world_models'].append(float(wm_prob_non_star))\n",
    "\n",
    "def add_shadow_model_probs(value, y_star, y_non_star, prompt, client, shadow_model_endpoints):\n",
    "    if 'shadow_models' not in value['y_stars'][y_star]:\n",
    "        value['y_stars'][y_star]['shadow_models'] = []\n",
    "    if 'shadow_models' not in value['y_NON_stars'][y_non_star]:\n",
    "        value['y_NON_stars'][y_non_star]['shadow_models'] = []\n",
    "        \n",
    "    for sm_endpoint in tqdm(shadow_model_endpoints, desc=\"shadow model endpoints\", leave=False):\n",
    "        sm_prob_gen = GenerateNextTokenProbAPI(client, sm_endpoint)\n",
    "        sm_prob_star = compute_token_probs_api(y_star, prompt, sm_prob_gen, max_tokens)\n",
    "        sm_prob_non_star = compute_token_probs_api(y_non_star, prompt, sm_prob_gen, max_tokens)\n",
    "        value['y_stars'][y_star]['shadow_models'].append(float(sm_prob_star))\n",
    "        value['y_NON_stars'][y_non_star]['shadow_models'].append(float(sm_prob_non_star))\n",
    "\n",
    "def add_model_probs(results, train_test_ents, client, world_model_endpoints, shadow_model_endpoints, model_type='world'):\n",
    "\n",
    "    def find_ent_list(dataset_type, sample_id):\n",
    "        for sample in train_test_ents[dataset_type]:\n",
    "            if sample['new_ID'] == sample_id:\n",
    "                return sample\n",
    "        return None\n",
    "\n",
    "    for key, value in tqdm(results.items(), desc=\"processing results\"):\n",
    "        split_key = key.split('_')\n",
    "        dataset_type = split_key[0]\n",
    "        sample_id = int(split_key[1])\n",
    "        ent_list = find_ent_list(dataset_type, sample_id)\n",
    "        if ent_list is None:\n",
    "            continue\n",
    "\n",
    "        ents = ent_list['ents']        \n",
    "        y_stars_order = list(value['y_stars'].keys())\n",
    "        y_non_stars_order = list(value['y_NON_stars'].keys())\n",
    "\n",
    "        for y_star, y_non_star in tqdm(zip(y_stars_order, y_non_stars_order), total=len(y_stars_order), desc=\"processing pairs\", leave=True):\n",
    "            if y_star not in ents:\n",
    "                continue\n",
    "            star_index = ents.index(y_star)\n",
    "            remaining_ents = ents[:star_index] + ents[star_index + 1:]\n",
    "            prompt_start = PROMPT_TEMPLATE[PROMPT_TO_USE][0]\n",
    "            prompt_end = PROMPT_TEMPLATE[PROMPT_TO_USE][1]\n",
    "            ents_string = ', '.join(remaining_ents)\n",
    "            prompt = f\"{prompt_start} {ents_string} {prompt_end}\"\n",
    "            max_tokens=len(prob_generator.tokenizer(prompt)['input_ids'])+10\n",
    "            \n",
    "            if model_type == 'world':\n",
    "                add_world_model_probs(value, y_star, y_non_star, prompt, client, world_model_api_keys, max_tokens)\n",
    "            else:\n",
    "                add_shadow_model_probs(value, y_star, y_non_star, prompt, client, shadow_model_api_keys, max_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a06f785f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token will not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\n",
      "Token is valid (permission: write).\n",
      "Your token has been saved to /Users/binhan/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "with open('model_map.json') as f:\n",
    "    model_map=json.load(f)\n",
    "key = '779d92de61a5035835e5023ca79e2e5b6124c6300c3ceb0e07e374f948554116'\n",
    "client = Together(api_key=key)\n",
    "hf_login(token=\"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\")\n",
    "dataset = load_dataset(\"beanham/medsum_llm_attack\")\n",
    "merged_dataset = concatenate_datasets([dataset['train'], dataset['validation'], dataset['test']])\n",
    "new_ids = range(len(merged_dataset))\n",
    "merged_dataset = merged_dataset.add_column(\"new_ID\", new_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "29f1bf4a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(146, 145, ['bh193/Meta-Llama-3.1-8B-Instruct-Reference-9082d8a1-9c66a72a'])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## load model\n",
    "id=2\n",
    "shadow_model_endpoints=[model_map['shadow_train'+'_'+str(id)]['api_key']]\n",
    "with open(f'target_token_probs_train_{id}_10_epochs.json', 'r') as f:\n",
    "    all_model_probs = json.load(f)\n",
    "ent_count_threshold=5\n",
    "\n",
    "## load data\n",
    "target_subsample_ids = pd.read_csv(f\"formatted_data/subsample_ids_{id}.csv\")['new_ID'].tolist()\n",
    "train_dataset = merged_dataset.filter(lambda example: example['new_ID'] in target_subsample_ids)\n",
    "test_dataset = merged_dataset.filter(lambda example: example['new_ID'] not in target_subsample_ids)\n",
    "\n",
    "## why are we only using len(ents)<5 as the unseen ents?\n",
    "unseen_ents = [sample['disease_ents'] for sample in test_dataset if len(sample['disease_ents']) < ent_count_threshold]\n",
    "unseen_ents = [item for sublist in unseen_ents for item in sublist]\n",
    "\n",
    "train_dataset = [sample for sample in train_dataset if len(sample['disease_ents']) >= ent_count_threshold]\n",
    "test_dataset = [sample for sample in test_dataset if len(sample['disease_ents']) >= ent_count_threshold]\n",
    "train_test_ents = {'train': train_dataset,'test': test_dataset}\n",
    "len(train_dataset), len(test_dataset), shadow_model_endpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5836597d",
   "metadata": {},
   "outputs": [],
   "source": [
    "add_model_probs(all_model_probs, train_test_ents, client, [], shadow_model_endpoints, model_type='shadow')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e13b546",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'with_world_model_shadow_model_3.json', 'w') as f:\n",
    "    json.dump(all_model_probs, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

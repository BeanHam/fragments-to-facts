{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a9ca44e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from together import Together\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "from utils import *\n",
    "from huggingface_hub import login as hf_login\n",
    "from peft import prepare_model_for_kbit_training\n",
    "from datasets import concatenate_datasets, DatasetDict, load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, AutoModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a3564ebe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shadow models: [('lr2872/Meta-Llama-3.1-8B-Instruct-Reference-dd180cb8-b4b00ce6', 'meta-llama/Meta-Llama-3-8B-Instruct-3', [54, 831, 113, 355, 927, 565, 797, 755, 111, 127, 429, 75, 409, 89, 380, 829, 923, 134, 263, 126, 261, 847, 1013, 1095, 64, 128, 681, 933, 3, 153, 796, 28, 1029, 1012, 275, 896, 143, 1157, 146, 384, 764, 352, 164, 1062, 1156, 867, 316, 709, 185, 59, 48, 255, 193, 733, 720, 28, 90, 303, 106, 144, 910, 387, 685, 794, 145, 14, 597, 230, 1, 636, 124, 1020, 182, 1124, 484, 445, 489, 386, 94, 804, 531, 11, 856, 444, 574, 147, 4, 108, 57, 613, 16, 703, 348, 18, 640, 107, 151, 578, 25, 287, 130, 1091, 315, 35, 152, 87, 364, 1090, 295, 1075, 165, 893, 936, 353, 612, 1135, 707, 619, 974, 18, 45, 630, 1155, 2, 65, 168, 112, 36, 940, 246, 621, 76, 1103, 562, 163, 656, 138, 939, 163, 932, 286, 596, 0, 692, 725, 1153, 190, 1068, 758, 911, 1046, 6, 235, 126, 60, 136, 5, 47, 563, 176, 1167, 851, 68, 81, 32, 1088, 44, 349, 453, 766, 795, 536, 491, 115, 616, 653, 1051, 1158, 969, 101, 254, 719, 66, 931, 5, 708, 515, 1009, 1101, 1042, 22, 576, 887, 33, 1018, 306, 92, 762, 379, 80, 101, 41, 861, 862, 761, 628, 53, 305, 141, 371, 466, 1079, 555, 902, 872, 1047, 432, 539, 1174, 37, 72, 727, 216, 105, 182, 483, 66, 82, 881, 894, 1197, 899, 1080, 183, 59, 173, 501, 140, 293, 875, 783, 343, 516, 418, 25, 247, 79, 570, 682, 715, 302, 52, 120, 30, 131, 64, 51, 13, 22, 68, 626, 922, 189, 822, 1025, 264, 63, 276, 631, 660, 753, 317, 840, 297, 38, 451, 741, 949, 454, 878, 972, 179, 314, 778, 1065, 38, 22, 178, 663, 785, 417, 1184, 188, 112, 98, 31, 497, 96, 42, 502, 237, 120, 165, 748, 34, 521, 0, 1139, 544, 319, 767, 624, 34, 545, 1146, 117, 149, 472, 416, 26, 498, 691, 803, 62, 1070, 307, 36, 1053, 816, 86, 1098, 960, 693, 610, 8, 158, 828, 109, 780, 477, 1067, 202, 1151, 870, 734, 581, 27, 280, 728, 8, 907, 638, 976, 966, 158, 133, 446, 71, 214, 818, 170, 1023, 186, 909, 504, 140, 730, 611, 19, 510, 864, 197, 903, 614, 1200, 34, 155, 12, 55, 136, 10, 53, 989, 419, 148, 84, 743, 18, 1035, 599, 101, 914, 40, 426, 252, 756, 152, 199, 646, 1, 664, 739, 111, 14, 465, 1049, 185, 196, 360, 957, 714, 1072, 218, 127, 111, 313, 196, 241, 95, 1115, 900, 1163, 87, 633, 110, 73, 41, 11, 722, 1048, 645, 109, 1137, 326, 915, 23, 84, 144, 843, 192, 24, 430, 187, 7, 152, 533, 538, 154, 815, 149, 153, 107, 356, 740, 181, 208, 29, 462, 557, 3, 637, 391, 1161, 128, 968, 176, 1119, 876, 99, 63, 1145, 60, 54, 1188, 70, 1126, 184, 122, 823, 108, 79, 821, 61, 110, 49, 1132, 883, 37, 103, 74, 103, 584, 75, 72, 8, 251, 3, 979, 161, 131, 93, 174, 591, 158, 474, 603, 679, 978, 83, 153, 74, 1005, 567, 997, 948, 928, 1010, 689, 143, 26, 71, 262, 404, 1110, 895, 169, 532, 982, 970, 279, 21, 1026, 977, 854, 807, 648, 207, 742, 586, 657, 790, 257, 69, 643, 991, 639, 905, 513, 180, 808, 55, 16, 953, 1059, 90, 310, 196, 65, 184, 125, 354, 128, 161, 464, 350, 67, 45, 589, 143, 993, 248, 61, 839, 89, 996, 43, 363, 805, 1193, 434, 21, 5, 763, 194, 179, 40, 601, 1052, 677, 479, 48, 428, 132, 367, 1033, 65, 70, 225, 1004, 157, 560, 768, 874, 0, 134, 157, 85, 339, 59, 580, 540, 129, 1128, 525, 788, 92, 191, 81, 113, 170, 503, 607, 318, 14, 8, 94, 126, 164, 97, 992, 277, 704, 747, 943, 1074, 119, 726, 79, 27, 2, 366, 331, 924, 171, 30, 308, 590, 1113, 1191, 706, 12, 1014, 520, 6, 21, 1141, 806, 70, 296, 46, 289, 232, 680, 167])]\n"
     ]
    }
   ],
   "source": [
    "## login & load together ai client\n",
    "# key = '779d92de61a5035835e5023ca79e2e5b6124c6300c3ceb0e07e374f948554116'\n",
    "key = 'e94217f61953b12489a9877936bd5383086106ec9951d3f11bb6a9475d88e95e'\n",
    "client = Together(api_key=key)\n",
    "hf_login(token=\"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\")\n",
    "\n",
    "## load dataset\n",
    "\n",
    "dataset = load_dataset(\"beanham/medsum_privacy\")\n",
    "merged_dataset = concatenate_datasets([dataset['train'], dataset['validation'], dataset['test']])\n",
    "\n",
    "api_keys_subsample_ids = [\n",
    "    # ('lr2872/Meta-Llama-3.1-8B-Instruct-Reference-dc7e8be7-f7a4e861','4')\n",
    "    ('lr2872/Meta-Llama-3.1-8B-Instruct-Reference-dd180cb8-b4b00ce6', '3')\n",
    "]\n",
    "# lr2872/Meta-Llama-3.1-8B-Instruct-Reference-2968ad77-11f16750 4\n",
    "# lr2872/Meta-Llama-3.1-8B-Instruct-Reference-baf1323a-aaf0f7d2 3\n",
    "\n",
    "# 4 epochs\n",
    "# lr2872/Meta-Llama-3.1-8B-Instruct-Reference-dc7e8be7-f7a4e861 4\n",
    "\n",
    "shadow_models = load_shadow_models_for_llama_3_instruct(MODEL_DICT, api_keys_subsample_ids)\n",
    "model_world = MODEL_DICT['llama_3_1_instruct']\n",
    "target_model_class = 'llama_3_1_instruct'\n",
    "print(f\"Shadow models: {shadow_models}\")\n",
    "target_model_api_key, _, target_subsample_ids = shadow_models[0]\n",
    "shadow_models_tuples = shadow_models[1:]\n",
    "shadow_model_api_keys = [api_key for api_key, _, _ in shadow_models_tuples]\n",
    "\n",
    "train_dataset = merged_dataset.filter(lambda example: example['ID'] in target_subsample_ids)\n",
    "test_dataset = merged_dataset.filter(lambda example: example['ID'] not in target_subsample_ids)\n",
    "unseen_ents = [sample['ents'] for sample in test_dataset if len(sample['ents']) < 5]\n",
    "unseen_ents = [item for sublist in unseen_ents for item in sublist]\n",
    "train_dataset = [sample for sample in train_dataset if len(sample['ents']) >= 5]\n",
    "test_dataset = [sample for sample in test_dataset if len(sample['ents']) >= 5]\n",
    "train_test_ents = {\n",
    "    'train': train_dataset,\n",
    "    'test': test_dataset\n",
    "}\n",
    "\n",
    "fake_train_test_ents = {\n",
    "    'train': train_dataset[0:1],\n",
    "    'test': test_dataset[0:1]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e4c29e0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "prob_generator = GenerateNextTokenProbAPI(client, target_model_api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1ab534a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running train\n",
      "running {'ID': 0, 'section_header': 'history of present illness', 'section_text': 'The patient is a 76-year-old white female who presents to the clinic today originally for hypertension and a med check.  She has a history of hypertension, osteoarthritis, osteoporosis, hypothyroidism, allergic rhinitis and kidney stones.  Since her last visit she has been followed by Dr. Kumar.  Those issues are stable.  She has had no fever or chills, cough, congestion, nausea, vomiting, chest pain, chest pressure.', 'dialogue': 'Doctor: What brings you back into the clinic today, miss? \\nPatient: I came in for a refill of my blood pressure medicine. \\nDoctor: It looks like Doctor Kumar followed up with you last time regarding your hypertension, osteoarthritis, osteoporosis, hypothyroidism, allergic rhinitis and kidney stones.  Have you noticed any changes or do you have any concerns regarding these issues?  \\nPatient: No. \\nDoctor: Have you had any fever or chills, cough, congestion, nausea, vomiting, chest pain, chest pressure?\\nPatient: No.  \\nDoctor: Great. Also, for our records, how old are you and what race do you identify yourself as?\\nPatient: I am seventy six years old and identify as a white female.', 'ents': ['osteoporosis', 'allergic rhinitis', 'hypothyroidism', 'osteoarthritis', 'kidney stones', 'fever', 'nausea,', 'chest pain', 'hypertension', 'cough', 'chills']}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/11 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "consider someone with the following conditions:  chest pain, kidney stones, cough, hypertension, nausea,, osteoarthritis, fever, chills, osteoporosis, allergic rhinitis . the individual then also has the condition \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "InvalidRequestError",
     "evalue": "Error code: 400 - {\"message\": \"The dedicated endpoint for lr2872/Meta-Llama-3.1-8B-Instruct-Reference-dd180cb8-b4b00ce6 is not running. Please visit https://api.together.ai/models to start the endpoint.\", \"type_\": \"invalid_request_error\", \"code\": \"dedicated_endpoint_not_running\"}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidRequestError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 37\u001b[0m\n\u001b[1;32m     33\u001b[0m prompt \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mprompt_start\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00ments_string\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mprompt_end\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28mprint\u001b[39m(prompt)\n\u001b[0;32m---> 37\u001b[0m prob \u001b[38;5;241m=\u001b[39m \u001b[43mcompute_token_probs_api\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_star\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprob_generator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     38\u001b[0m prob_NON \u001b[38;5;241m=\u001b[39m compute_token_probs_api(y_NON_star, prompt, prob_generator)            \n\u001b[1;32m     39\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m prob \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m prob_NON \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[0;32m~/Documents/extraction_attack_clones/clone2/2024-llm-attack/utils.py:134\u001b[0m, in \u001b[0;36mcompute_token_probs_api\u001b[0;34m(y_star_string, prompt, prob_generator)\u001b[0m\n\u001b[1;32m    133\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_token_probs_api\u001b[39m(y_star_string, prompt, prob_generator):    \n\u001b[0;32m--> 134\u001b[0m     prob \u001b[38;5;241m=\u001b[39m \u001b[43mprob_generator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_next_token_prob\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_star_string\u001b[49m\u001b[43m)\u001b[49m    \n\u001b[1;32m    135\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m prob\n",
      "File \u001b[0;32m~/Documents/extraction_attack_clones/clone2/2024-llm-attack/utils.py:110\u001b[0m, in \u001b[0;36mGenerateNextTokenProbAPI.get_next_token_prob\u001b[0;34m(self, input_string, target_string)\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_next_token_prob\u001b[39m(\u001b[38;5;28mself\u001b[39m, input_string, target_string):\n\u001b[1;32m    105\u001b[0m     messages \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    106\u001b[0m         {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msystem\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou are a medical expert.\u001b[39m\u001b[38;5;124m\"\u001b[39m},\n\u001b[1;32m    107\u001b[0m         {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: input_string \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m target_string}\n\u001b[1;32m    108\u001b[0m     ]\n\u001b[0;32m--> 110\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapi_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchat\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompletions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    111\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    112\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    113\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    114\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    115\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogprobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    116\u001b[0m \u001b[43m        \u001b[49m\u001b[43mecho\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[1;32m    117\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    119\u001b[0m     tokens \u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39mprompt[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mlogprobs\u001b[38;5;241m.\u001b[39mtokens\n\u001b[1;32m    120\u001b[0m     logprobs \u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39mprompt[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mlogprobs\u001b[38;5;241m.\u001b[39mtoken_logprobs\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/llm_attacks/lib/python3.10/site-packages/together/resources/chat/completions.py:141\u001b[0m, in \u001b[0;36mChatCompletions.create\u001b[0;34m(self, messages, model, max_tokens, stop, temperature, top_p, top_k, repetition_penalty, presence_penalty, frequency_penalty, min_p, logit_bias, seed, stream, logprobs, echo, n, safety_model, response_format, tools, tool_choice, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m requestor \u001b[38;5;241m=\u001b[39m api_requestor\u001b[38;5;241m.\u001b[39mAPIRequestor(\n\u001b[1;32m    113\u001b[0m     client\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_client,\n\u001b[1;32m    114\u001b[0m )\n\u001b[1;32m    116\u001b[0m parameter_payload \u001b[38;5;241m=\u001b[39m ChatCompletionRequest(\n\u001b[1;32m    117\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m    118\u001b[0m     messages\u001b[38;5;241m=\u001b[39mmessages,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    138\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    139\u001b[0m )\u001b[38;5;241m.\u001b[39mmodel_dump(exclude_none\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m--> 141\u001b[0m response, _, _ \u001b[38;5;241m=\u001b[39m \u001b[43mrequestor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    142\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mTogetherRequest\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    143\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mPOST\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    144\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mchat/completions\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    145\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparameter_payload\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    146\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    147\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    148\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    150\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m stream:\n\u001b[1;32m    151\u001b[0m     \u001b[38;5;66;03m# must be an iterator\u001b[39;00m\n\u001b[1;32m    152\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response, TogetherResponse)\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/llm_attacks/lib/python3.10/site-packages/together/abstract/api_requestor.py:249\u001b[0m, in \u001b[0;36mAPIRequestor.request\u001b[0;34m(self, options, stream, remaining_retries, request_timeout)\u001b[0m\n\u001b[1;32m    231\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrequest\u001b[39m(\n\u001b[1;32m    232\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    233\u001b[0m     options: TogetherRequest,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    240\u001b[0m     \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    241\u001b[0m ]:\n\u001b[1;32m    242\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequest_raw(\n\u001b[1;32m    243\u001b[0m         options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[1;32m    244\u001b[0m         remaining_retries\u001b[38;5;241m=\u001b[39mremaining_retries \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mretries,\n\u001b[1;32m    245\u001b[0m         stream\u001b[38;5;241m=\u001b[39mstream,\n\u001b[1;32m    246\u001b[0m         request_timeout\u001b[38;5;241m=\u001b[39mrequest_timeout,\n\u001b[1;32m    247\u001b[0m     )\n\u001b[0;32m--> 249\u001b[0m     resp, got_stream \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_interpret_response\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    250\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m resp, got_stream, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapi_key\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/llm_attacks/lib/python3.10/site-packages/together/abstract/api_requestor.py:632\u001b[0m, in \u001b[0;36mAPIRequestor._interpret_response\u001b[0;34m(self, result, stream)\u001b[0m\n\u001b[1;32m    624\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[1;32m    625\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_interpret_response_line(\n\u001b[1;32m    626\u001b[0m             line, result\u001b[38;5;241m.\u001b[39mstatus_code, result\u001b[38;5;241m.\u001b[39mheaders, stream\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    627\u001b[0m         )\n\u001b[1;32m    628\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m parse_stream(result\u001b[38;5;241m.\u001b[39miter_lines())\n\u001b[1;32m    629\u001b[0m     ), \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    630\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    631\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[0;32m--> 632\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_interpret_response_line\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    633\u001b[0m \u001b[43m            \u001b[49m\u001b[43mresult\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcontent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    634\u001b[0m \u001b[43m            \u001b[49m\u001b[43mresult\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstatus_code\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    635\u001b[0m \u001b[43m            \u001b[49m\u001b[43mresult\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    636\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    637\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m    638\u001b[0m         \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    639\u001b[0m     )\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/llm_attacks/lib/python3.10/site-packages/together/abstract/api_requestor.py:701\u001b[0m, in \u001b[0;36mAPIRequestor._interpret_response_line\u001b[0;34m(self, rbody, rcode, rheaders, stream)\u001b[0m\n\u001b[1;32m    699\u001b[0m \u001b[38;5;66;03m# Handle streaming errors\u001b[39;00m\n\u001b[1;32m    700\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;241m200\u001b[39m \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m rcode \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m300\u001b[39m:\n\u001b[0;32m--> 701\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandle_error_response(resp, rcode, stream_error\u001b[38;5;241m=\u001b[39mstream)\n\u001b[1;32m    702\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "\u001b[0;31mInvalidRequestError\u001b[0m: Error code: 400 - {\"message\": \"The dedicated endpoint for lr2872/Meta-Llama-3.1-8B-Instruct-Reference-dd180cb8-b4b00ce6 is not running. Please visit https://api.together.ai/models to start the endpoint.\", \"type_\": \"invalid_request_error\", \"code\": \"dedicated_endpoint_not_running\"}"
     ]
    }
   ],
   "source": [
    "results={}\n",
    "fail_counter = 0\n",
    "# get first 50 pairs from train_test_ents \n",
    "\n",
    "for name, samples in train_test_ents.items():\n",
    "    print(f\"running {name}\")\n",
    "    \n",
    "    for ent_list in samples[:25]:\n",
    "        print(f\"running {ent_list}\")\n",
    "        \n",
    "        ## create saving dictionary\n",
    "        key=name+'_'+str(ent_list['ID'])\n",
    "        results[key]={}\n",
    "        results[key]['y_stars']={}\n",
    "        results[key]['y_NON_stars']={}        \n",
    "        ents = list(set(ent_list['ents']))\n",
    "        k = len(ents)\n",
    "        unseen_ents_for_sample = random.sample(unseen_ents, k)\n",
    "        \n",
    "        ## go through each y_star\n",
    "        for i in tqdm(range(k)):\n",
    "            \n",
    "            y_star = ents[i]\n",
    "            y_NON_star = unseen_ents_for_sample[i]\n",
    "            results[key]['y_stars'][y_star]={}\n",
    "            results[key]['y_NON_stars'][y_NON_star]={}                        \n",
    "            remaining_ents = ents[:i] + ents[i + 1:]            \n",
    "            \n",
    "            # prompt = f\"consider someone with the following conditions: {', '.join(remaining_ents)}. the individual then also has the condition \"\n",
    "            prompt_start = PROMPT_TEMPLATE[PROMPT_TO_USE][0]\n",
    "            prompt_end = PROMPT_TEMPLATE[PROMPT_TO_USE][1]\n",
    "            ents_string = ', '.join(remaining_ents)\n",
    "            prompt = f\"{prompt_start} {ents_string} {prompt_end}\"\n",
    "\n",
    "            print(prompt)\n",
    "\n",
    "            prob = compute_token_probs_api(y_star, prompt, prob_generator) \n",
    "            prob_NON = compute_token_probs_api(y_NON_star, prompt, prob_generator)            \n",
    "            if prob == -1 or prob_NON == -1:\n",
    "                fail_counter += 1\n",
    "                print(f\"failed {fail_counter} times\")\n",
    "                continue            \n",
    "            results[key]['y_stars'][y_star]['target']=prob\n",
    "            results[key]['y_NON_stars'][y_NON_star]['target']=prob_NON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bc621fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'target_token_probs_3_prompt_0_4_epochs.json', 'w') as f:\n",
    "    json.dump(results, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9de1dffb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_attacks",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

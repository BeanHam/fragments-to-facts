You are about to create a fine-tuning job. The cost of your job will be determined by the model size, the number of tokens in the training file, the number of tokens in the validation file, the number of epochs, and the number of evaluations. Visit https://www.together.ai/pricing to get a price estimate.
You can pass `-y` or `--confirm` to your command to skip this message.

Do you want to proceed? [Y/n]: message='Starting from together>=1.3.0, the default batch size is set to the maximum allowed value for each model.'
message='Starting from together>=1.3.0, the default batch size is set to the maximum allowed value for each model.'
Submitting a fine-tuning job with the following parameters:
FinetuneRequest(
    training_file='file-8fabee7f-0b2c-4b1a-a19e-85025d75adbe',
    validation_file='file-925a1a42-62b3-4a43-a03a-084322b359e2',
    model='meta-llama/Llama-3.2-1B-Instruct',
    n_epochs=20,
    learning_rate=1e-05,
    lr_scheduler=FinetuneLRScheduler(
        lr_scheduler_type='linear',
        lr_scheduler_args=FinetuneLinearLRSchedulerArgs(min_lr_ratio=0.0)
    ),
    warmup_ratio=0.0,
    max_grad_norm=1.0,
    weight_decay=0.0,
    n_checkpoints=1,
    n_evals=10,
    batch_size=40,
    suffix=None,
    wandb_key='a73070a2ae35aa73562604c69dfc697278d19086',
    wandb_base_url=None,
    wandb_project_name=None,
    wandb_name=None,
    training_type=LoRATrainingType(
        type='Lora',
        lora_r=8,
        lora_alpha=16,
        lora_dropout=0.0,
        lora_trainable_modules='all-linear'
    ),
    train_on_inputs='auto'
)
Successfully submitted a fine-tuning job ft-76de7dbf at 02/20/2025, 21:00:06

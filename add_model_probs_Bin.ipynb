{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9c7ee2ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/binhan/anaconda3/lib/python3.11/site-packages/bitsandbytes/cextension.py:34: UserWarning: The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers, 8-bit multiplication, and GPU quantization are unavailable.\n",
      "  warn(\"The installed version of bitsandbytes was compiled without GPU support. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'NoneType' object has no attribute 'cadam32bit_grad_fp32'\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from together import Together\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "from utils import *\n",
    "from huggingface_hub import login as hf_login\n",
    "from peft import prepare_model_for_kbit_training\n",
    "from datasets import concatenate_datasets, DatasetDict, load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, AutoModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "09a052b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROMPT_TO_USE = 0\n",
    "def add_world_model_probs(value, y_star, y_non_star, prompt, client, world_model_endpoints):\n",
    "    if 'world_models' not in value['y_stars'][y_star]:\n",
    "        value['y_stars'][y_star]['world_models'] = []\n",
    "    if 'world_models' not in value['y_NON_stars'][y_non_star]:\n",
    "        value['y_NON_stars'][y_non_star]['world_models'] = []\n",
    "        \n",
    "    for wm_endpoint in tqdm(world_model_endpoints, desc=\"world model endpoints\", leave=False):\n",
    "        wm_prob_gen = GenerateNextTokenProbAPI(client, wm_endpoint)\n",
    "        max_tokens=len(wm_prob_gen.tokenizer(prompt)['input_ids'])+10\n",
    "        wm_prob_star = compute_token_probs_api(y_star, prompt, wm_prob_gen, max_tokens)\n",
    "        wm_prob_non_star = compute_token_probs_api(y_non_star, prompt, wm_prob_gen, max_tokens)\n",
    "        value['y_stars'][y_star]['world_models'].append(float(wm_prob_star))\n",
    "        value['y_NON_stars'][y_non_star]['world_models'].append(float(wm_prob_non_star))\n",
    "\n",
    "def add_shadow_model_probs(value, y_star, y_non_star, prompt, client, shadow_model_endpoints):\n",
    "    if 'shadow_models' not in value['y_stars'][y_star]:\n",
    "        value['y_stars'][y_star]['shadow_models'] = []\n",
    "    if 'shadow_models' not in value['y_NON_stars'][y_non_star]:\n",
    "        value['y_NON_stars'][y_non_star]['shadow_models'] = []\n",
    "        \n",
    "    for sm_endpoint in tqdm(shadow_model_endpoints, desc=\"shadow model endpoints\", leave=False):\n",
    "        sm_prob_gen = GenerateNextTokenProbAPI(client, sm_endpoint)\n",
    "        max_tokens=len(sm_prob_gen.tokenizer(prompt)['input_ids'])+10\n",
    "        sm_prob_star = compute_token_probs_api(y_star, prompt, sm_prob_gen, max_tokens)\n",
    "        sm_prob_non_star = compute_token_probs_api(y_non_star, prompt, sm_prob_gen, max_tokens)\n",
    "        value['y_stars'][y_star]['shadow_models'].append(float(sm_prob_star))\n",
    "        value['y_NON_stars'][y_non_star]['shadow_models'].append(float(sm_prob_non_star))\n",
    "\n",
    "def add_model_probs(results, train_test_ents, client, world_model_endpoints, shadow_model_endpoints, model_type='world'):\n",
    "\n",
    "    def find_ent_list(dataset_type, sample_id):\n",
    "        for sample in train_test_ents[dataset_type]:\n",
    "            if sample['new_ID'] == sample_id:\n",
    "                return sample\n",
    "        return None\n",
    "\n",
    "    for key, value in tqdm(results.items(), desc=\"processing results\"):\n",
    "        split_key = key.split('_')\n",
    "        dataset_type = split_key[0]\n",
    "        sample_id = int(split_key[1])\n",
    "        ent_list = find_ent_list(dataset_type, sample_id)\n",
    "        if ent_list is None:\n",
    "            continue\n",
    "\n",
    "        ents = ent_list['disease_ents']        \n",
    "        y_stars_order = list(value['y_stars'].keys())\n",
    "        y_non_stars_order = list(value['y_NON_stars'].keys())\n",
    "\n",
    "        for y_star, y_non_star in tqdm(zip(y_stars_order, y_non_stars_order), total=len(y_stars_order), desc=\"processing pairs\", leave=True):\n",
    "            if y_star not in ents:\n",
    "                continue\n",
    "            star_index = ents.index(y_star)\n",
    "            remaining_ents = ents[:star_index] + ents[star_index + 1:]\n",
    "            prompt_start = PROMPT_TEMPLATE[PROMPT_TO_USE][0]\n",
    "            prompt_end = PROMPT_TEMPLATE[PROMPT_TO_USE][1]\n",
    "            ents_string = ', '.join(remaining_ents)\n",
    "            prompt = f\"{prompt_start} {ents_string} {prompt_end}\"            \n",
    "            \n",
    "            if model_type == 'world':\n",
    "                add_world_model_probs(value, y_star, y_non_star, prompt, client, world_model_endpoints)\n",
    "            else:\n",
    "                add_shadow_model_probs(value, y_star, y_non_star, prompt, client, shadow_model_endpoints)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a06f785f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token will not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\n",
      "Token is valid (permission: write).\n",
      "Your token has been saved to /Users/binhan/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "with open('model_map.json') as f:\n",
    "    model_map=json.load(f)\n",
    "key = '779d92de61a5035835e5023ca79e2e5b6124c6300c3ceb0e07e374f948554116'\n",
    "client = Together(api_key=key)\n",
    "hf_login(token=\"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\")\n",
    "dataset = load_dataset(\"beanham/medsum_llm_attack\")\n",
    "merged_dataset = concatenate_datasets([dataset['train'], dataset['validation'], dataset['test']])\n",
    "new_ids = range(len(merged_dataset))\n",
    "merged_dataset = merged_dataset.add_column(\"new_ID\", new_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "29f1bf4a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(146, 145, ['bh193/Meta-Llama-3.1-8B-Instruct-Reference-9082d8a1-9c66a72a'])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## load model\n",
    "id=2\n",
    "shadow_model_endpoints=[model_map['shadow_train'+'_'+str(id)]['api_key']]\n",
    "with open(f'target_token_probs_train_{id}_10_epochs.json', 'r') as f:\n",
    "    all_model_probs = json.load(f)\n",
    "ent_count_threshold=5\n",
    "\n",
    "## load data\n",
    "target_subsample_ids = pd.read_csv(f\"formatted_data/subsample_ids_{id}.csv\")['new_ID'].tolist()\n",
    "train_dataset = merged_dataset.filter(lambda example: example['new_ID'] in target_subsample_ids)\n",
    "test_dataset = merged_dataset.filter(lambda example: example['new_ID'] not in target_subsample_ids)\n",
    "\n",
    "## why are we only using len(ents)<5 as the unseen ents?\n",
    "unseen_ents = [sample['disease_ents'] for sample in test_dataset if len(sample['disease_ents']) < ent_count_threshold]\n",
    "unseen_ents = [item for sublist in unseen_ents for item in sublist]\n",
    "\n",
    "train_dataset = [sample for sample in train_dataset if len(sample['disease_ents']) >= ent_count_threshold]\n",
    "test_dataset = [sample for sample in test_dataset if len(sample['disease_ents']) >= ent_count_threshold]\n",
    "train_test_ents = {'train': train_dataset,'test': test_dataset}\n",
    "len(train_dataset), len(test_dataset), shadow_model_endpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5836597d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "processing results:   0%|                               | 0/283 [00:00<?, ?it/s]\n",
      "processing pairs:   0%|                                   | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      "shadow model endpoints:   0%|                             | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[ASpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "\n",
      "\n",
      "shadow model endpoints: 100%|█████████████████████| 1/1 [00:02<00:00,  2.83s/it]\u001b[A\u001b[A\n",
      "\n",
      "                                                                                \u001b[A\u001b[A\n",
      "processing pairs: 100%|███████████████████████████| 5/5 [00:02<00:00,  1.74it/s]\u001b[A\n",
      "processing results:   2%|▍                      | 6/283 [00:02<02:12,  2.09it/s]\n",
      "processing pairs:  88%|█████████████████████   | 7/8 [00:00<00:00, 20531.56it/s]\u001b[A\n",
      "\n",
      "processing pairs:  83%|████████████████████▊    | 5/6 [00:00<00:00, 8195.20it/s]\u001b[A\n",
      "\n",
      "processing pairs: 100%|█████████████████████| 11/11 [00:00<00:00, 124695.52it/s]\u001b[A\n",
      "\n",
      "processing pairs: 100%|████████████████████████| 5/5 [00:00<00:00, 57456.22it/s]\u001b[A\n",
      "\n",
      "processing pairs:   0%|                                   | 0/6 [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      "shadow model endpoints:   0%|                             | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[ASpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "\n",
      "\n",
      "shadow model endpoints: 100%|█████████████████████| 1/1 [00:02<00:00,  2.50s/it]\u001b[A\u001b[A\n",
      "\n",
      "                                                                                \u001b[A\u001b[A\n",
      "processing pairs:  50%|█████████████▌             | 3/6 [00:02<00:02,  1.18it/s]\u001b[A\n",
      "\n",
      "shadow model endpoints:   0%|                             | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[ASpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "\n",
      "\n",
      "shadow model endpoints: 100%|█████████████████████| 1/1 [00:02<00:00,  2.42s/it]\u001b[A\u001b[A\n",
      "\n",
      "                                                                                \u001b[A\u001b[A\n",
      "processing pairs:  83%|██████████████████████▌    | 5/6 [00:04<00:01,  1.03s/it]\u001b[A\n",
      "\n",
      "shadow model endpoints:   0%|                             | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[ASpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "\n",
      "\n",
      "shadow model endpoints: 100%|█████████████████████| 1/1 [00:02<00:00,  2.17s/it]\u001b[A\u001b[A\n",
      "\n",
      "                                                                                \u001b[A\u001b[A\n",
      "processing pairs: 100%|███████████████████████████| 6/6 [00:07<00:00,  1.20s/it]\u001b[A\n",
      "processing results:  22%|████▉                 | 63/283 [00:10<00:33,  6.62it/s]\n",
      "processing pairs:   0%|                                   | 0/9 [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      "shadow model endpoints:   0%|                             | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[ASpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "\n",
      "\n",
      "shadow model endpoints: 100%|█████████████████████| 1/1 [00:02<00:00,  2.26s/it]\u001b[A\u001b[A\n",
      "\n",
      "                                                                                \u001b[A\u001b[A\n",
      "processing pairs:  22%|██████                     | 2/9 [00:02<00:08,  1.14s/it]\u001b[A\n",
      "\n",
      "shadow model endpoints:   0%|                             | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[ASpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "\n",
      "\n",
      "shadow model endpoints: 100%|█████████████████████| 1/1 [00:02<00:00,  2.53s/it]\u001b[A\u001b[A\n",
      "\n",
      "                                                                                \u001b[A\u001b[A\n",
      "processing pairs:  89%|████████████████████████   | 8/9 [00:04<00:00,  1.65it/s]\u001b[A\n",
      "processing results:  25%|█████▌                | 72/283 [00:14<00:46,  4.57it/s]\n",
      "processing pairs: 100%|████████████████████████| 5/5 [00:00<00:00, 70138.86it/s]\u001b[A\n",
      "\n",
      "processing pairs: 100%|████████████████████████| 5/5 [00:00<00:00, 70611.18it/s]\u001b[A\n",
      "\n",
      "processing pairs: 100%|███████████████████████| 9/9 [00:00<00:00, 108162.57it/s]\u001b[A\n",
      "\n",
      "processing pairs: 100%|████████████████████████| 7/7 [00:00<00:00, 32406.32it/s]\u001b[A\n",
      "\n",
      "processing pairs:   0%|                                  | 0/11 [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      "shadow model endpoints:   0%|                             | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[ASpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "\n",
      "\n",
      "shadow model endpoints: 100%|█████████████████████| 1/1 [00:02<00:00,  2.29s/it]\u001b[A\u001b[A\n",
      "\n",
      "                                                                                \u001b[A\u001b[A\n",
      "processing pairs:   9%|██▎                       | 1/11 [00:02<00:23,  2.32s/it]\u001b[A\n",
      "\n",
      "shadow model endpoints:   0%|                             | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[ASpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "\n",
      "\n",
      "shadow model endpoints: 100%|█████████████████████| 1/1 [00:02<00:00,  2.52s/it]\u001b[A\u001b[A\n",
      "\n",
      "                                                                                \u001b[A\u001b[A\n",
      "processing pairs:  18%|████▋                     | 2/11 [00:04<00:22,  2.46s/it]\u001b[A\n",
      "\n",
      "shadow model endpoints:   0%|                             | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[ASpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "\n",
      "\n",
      "shadow model endpoints: 100%|█████████████████████| 1/1 [00:02<00:00,  2.06s/it]\u001b[A\u001b[A\n",
      "\n",
      "                                                                                \u001b[A\u001b[A\n",
      "processing pairs:  27%|███████                   | 3/11 [00:06<00:18,  2.29s/it]\u001b[A\n",
      "\n",
      "shadow model endpoints:   0%|                             | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[ASpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "\n",
      "\n",
      "shadow model endpoints: 100%|█████████████████████| 1/1 [00:02<00:00,  2.50s/it]\u001b[A\u001b[A\n",
      "\n",
      "                                                                                \u001b[A\u001b[A\n",
      "processing pairs:  36%|█████████▍                | 4/11 [00:09<00:16,  2.39s/it]\u001b[A\n",
      "\n",
      "shadow model endpoints:   0%|                             | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[ASpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "\n",
      "\n",
      "shadow model endpoints: 100%|█████████████████████| 1/1 [00:02<00:00,  2.04s/it]\u001b[A\u001b[A\n",
      "\n",
      "                                                                                \u001b[A\u001b[A\n",
      "processing pairs:  45%|███████████▊              | 5/11 [00:11<00:13,  2.27s/it]\u001b[A\n",
      "\n",
      "shadow model endpoints:   0%|                             | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[ASpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "\n",
      "\n",
      "shadow model endpoints: 100%|█████████████████████| 1/1 [00:02<00:00,  2.53s/it]\u001b[A\u001b[A\n",
      "\n",
      "                                                                                \u001b[A\u001b[A\n",
      "processing pairs:  55%|██████████████▏           | 6/11 [00:14<00:11,  2.37s/it]\u001b[A\n",
      "\n",
      "shadow model endpoints:   0%|                             | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[ASpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "\n",
      "\n",
      "shadow model endpoints: 100%|█████████████████████| 1/1 [00:02<00:00,  2.50s/it]\u001b[A\u001b[A\n",
      "\n",
      "                                                                                \u001b[A\u001b[A\n",
      "processing pairs:  64%|████████████████▌         | 7/11 [00:16<00:09,  2.42s/it]\u001b[A\n",
      "\n",
      "shadow model endpoints:   0%|                             | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[ASpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "\n",
      "\n",
      "shadow model endpoints: 100%|█████████████████████| 1/1 [00:02<00:00,  2.40s/it]\u001b[A\u001b[A\n",
      "\n",
      "                                                                                \u001b[A\u001b[A\n",
      "processing pairs:  73%|██████████████████▉       | 8/11 [00:19<00:07,  2.43s/it]\u001b[A\n",
      "\n",
      "shadow model endpoints:   0%|                             | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[ASpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "\n",
      "\n",
      "shadow model endpoints: 100%|█████████████████████| 1/1 [00:02<00:00,  2.65s/it]\u001b[A\u001b[A\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \u001b[A\u001b[A\n",
      "processing pairs:  82%|█████████████████████▎    | 9/11 [00:21<00:05,  2.51s/it]\u001b[A\n",
      "\n",
      "shadow model endpoints:   0%|                             | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[ASpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "\n",
      "\n",
      "shadow model endpoints: 100%|█████████████████████| 1/1 [00:02<00:00,  2.63s/it]\u001b[A\u001b[A\n",
      "\n",
      "                                                                                \u001b[A\u001b[A\n",
      "processing pairs:  91%|██████████████████████▋  | 10/11 [00:24<00:02,  2.55s/it]\u001b[A\n",
      "\n",
      "shadow model endpoints:   0%|                             | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[ASpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "\n",
      "\n",
      "shadow model endpoints: 100%|█████████████████████| 1/1 [00:02<00:00,  2.23s/it]\u001b[A\u001b[A\n",
      "\n",
      "                                                                                \u001b[A\u001b[A\n",
      "processing pairs: 100%|█████████████████████████| 11/11 [00:26<00:00,  2.43s/it]\u001b[A\n",
      "processing results:  51%|██████████▋          | 144/283 [00:41<00:44,  3.12it/s]\n",
      "processing pairs:   0%|                                   | 0/8 [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      "shadow model endpoints:   0%|                             | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[ASpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "\n",
      "\n",
      "shadow model endpoints: 100%|█████████████████████| 1/1 [00:02<00:00,  2.25s/it]\u001b[A\u001b[A\n",
      "\n",
      "                                                                                \u001b[A\u001b[A\n",
      "processing pairs:  88%|███████████████████████▋   | 7/8 [00:02<00:00,  3.07it/s]\u001b[A\n",
      "processing results:  51%|██████████▊          | 145/283 [00:43<00:47,  2.89it/s]\n",
      "processing pairs: 100%|███████████████████████| 8/8 [00:00<00:00, 101372.91it/s]\u001b[A\n",
      "\n",
      "processing pairs: 100%|████████████████████████| 5/5 [00:00<00:00, 69672.82it/s]\u001b[A\n",
      "\n",
      "processing pairs: 100%|████████████████████████| 5/5 [00:00<00:00, 72817.78it/s]\u001b[A\n",
      "\n",
      "processing pairs:   0%|                                   | 0/9 [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      "shadow model endpoints:   0%|                             | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[ASpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "\n",
      "\n",
      "shadow model endpoints: 100%|█████████████████████| 1/1 [00:02<00:00,  2.19s/it]\u001b[A\u001b[A\n",
      "\n",
      "                                                                                \u001b[A\u001b[A\n",
      "processing pairs:  67%|██████████████████         | 6/9 [00:02<00:01,  2.71it/s]\u001b[A\n",
      "\n",
      "shadow model endpoints:   0%|                             | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[ASpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "\n",
      "\n",
      "shadow model endpoints: 100%|█████████████████████| 1/1 [00:02<00:00,  2.23s/it]\u001b[A\u001b[A\n",
      "\n",
      "                                                                                \u001b[A\u001b[A\n",
      "processing pairs: 100%|███████████████████████████| 9/9 [00:04<00:00,  2.01it/s]\u001b[A\n",
      "processing results:  65%|█████████████▋       | 184/283 [00:48<00:24,  4.02it/s]\n",
      "processing pairs: 100%|████████████████████████| 7/7 [00:00<00:00, 35203.99it/s]\u001b[A\n",
      "\n",
      "processing pairs: 100%|████████████████████████| 7/7 [00:00<00:00, 46019.01it/s]\u001b[A\n",
      "\n",
      "processing pairs: 100%|████████████████████████| 6/6 [00:00<00:00, 16080.40it/s]\u001b[A\n",
      "\n",
      "processing pairs: 100%|███████████████████████| 9/9 [00:00<00:00, 111025.69it/s]\u001b[A\n",
      "\n",
      "processing pairs: 100%|████████████████████████| 5/5 [00:00<00:00, 75709.46it/s]\u001b[A\n",
      "\n",
      "processing pairs:   0%|                                  | 0/14 [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      "shadow model endpoints:   0%|                             | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[ASpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "\n",
      "\n",
      "shadow model endpoints: 100%|█████████████████████| 1/1 [00:02<00:00,  2.25s/it]\u001b[A\u001b[A\n",
      "\n",
      "                                                                                \u001b[A\u001b[A\n",
      "processing pairs:  79%|███████████████████▋     | 11/14 [00:02<00:00,  4.81it/s]\u001b[A\n",
      "processing results:  76%|███████████████▉     | 215/283 [00:50<00:13,  5.19it/s]\n",
      "processing pairs: 100%|████████████████████████| 6/6 [00:00<00:00, 77912.77it/s]\u001b[A\n",
      "\n",
      "processing pairs: 100%|████████████████████████| 7/7 [00:00<00:00, 39094.71it/s]\u001b[A\n",
      "\n",
      "processing pairs:   0%|                                   | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      "shadow model endpoints:   0%|                             | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[ASpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "\n",
      "\n",
      "shadow model endpoints: 100%|█████████████████████| 1/1 [00:02<00:00,  2.72s/it]\u001b[A\u001b[A\n",
      "\n",
      "                                                                                \u001b[A\u001b[A\n",
      "processing pairs: 100%|███████████████████████████| 5/5 [00:02<00:00,  1.81it/s]\u001b[A\n",
      "processing results:  81%|█████████████████    | 230/283 [00:53<00:10,  5.23it/s]\n",
      "processing pairs: 100%|█████████████████████| 13/13 [00:00<00:00, 161798.08it/s]\u001b[A\n",
      "\n",
      "processing pairs:   0%|                                   | 0/8 [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      "shadow model endpoints:   0%|                             | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[ASpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "\n",
      "\n",
      "shadow model endpoints: 100%|█████████████████████| 1/1 [00:02<00:00,  2.41s/it]\u001b[A\u001b[A\n",
      "\n",
      "                                                                                \u001b[A\u001b[A\n",
      "processing pairs:  62%|████████████████▉          | 5/8 [00:02<00:01,  2.05it/s]\u001b[A\n",
      "processing results:  85%|█████████████████▊   | 240/283 [00:56<00:08,  5.02it/s]\n",
      "processing pairs:   0%|                                  | 0/12 [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      "shadow model endpoints:   0%|                             | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[ASpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "\n",
      "\n",
      "shadow model endpoints: 100%|█████████████████████| 1/1 [00:02<00:00,  2.21s/it]\u001b[A\u001b[A\n",
      "\n",
      "                                                                                \u001b[A\u001b[A\n",
      "processing pairs:  33%|████████▋                 | 4/12 [00:02<00:04,  1.78it/s]\u001b[A\n",
      "\n",
      "shadow model endpoints:   0%|                             | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[ASpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "\n",
      "\n",
      "shadow model endpoints: 100%|█████████████████████| 1/1 [00:02<00:00,  2.24s/it]\u001b[A\u001b[A\n",
      "\n",
      "                                                                                \u001b[A\u001b[A\n",
      "processing pairs:  83%|████████████████████▊    | 10/12 [00:04<00:00,  2.21it/s]\u001b[A\n",
      "processing results:  88%|██████████████████▍  | 249/283 [01:00<00:08,  4.04it/s]\n",
      "processing pairs: 100%|████████████████████████| 5/5 [00:00<00:00, 12993.51it/s]\u001b[A\n",
      "\n",
      "processing pairs:   0%|                                  | 0/10 [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      "shadow model endpoints:   0%|                             | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[ASpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "\n",
      "\n",
      "shadow model endpoints: 100%|█████████████████████| 1/1 [00:02<00:00,  2.26s/it]\u001b[A\u001b[A\n",
      "\n",
      "                                                                                \u001b[A\u001b[A\n",
      "processing pairs: 100%|█████████████████████████| 10/10 [00:02<00:00,  4.35it/s]\u001b[A\n",
      "processing results:  91%|███████████████████▏ | 258/283 [01:02<00:06,  4.01it/s]\n",
      "processing pairs: 100%|██████████████████████| 10/10 [00:00<00:00, 60963.72it/s]\u001b[A\n",
      "\n",
      "processing pairs: 100%|█████████████████████| 11/11 [00:00<00:00, 136098.36it/s]\u001b[A\n",
      "processing results: 100%|█████████████████████| 283/283 [01:02<00:00,  4.50it/s]\n"
     ]
    }
   ],
   "source": [
    "add_model_probs(all_model_probs, train_test_ents, client, [], shadow_model_endpoints, model_type='shadow')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7e13b546",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'target_shadow_token_probs_train_{id}_10_epochs.json', 'w') as f:\n",
    "    json.dump(all_model_probs, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
